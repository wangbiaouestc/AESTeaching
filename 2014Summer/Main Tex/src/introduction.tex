The increasing demand of high quality video communication in nearly every part of the electronic entertainment area and the tremendous growth of video contents on the Internet pushed the development of high efficient compression methods. These codecs however are always limited by the average power of the computers used at there design time. \\
Especially with High Definition (HD) video content going to be replaced by beyond-HD content, like Ultra High Definition (UHD), a high compression rate is very important due to the limitations in transfer rate in many areas.\\
The H.264/AVC \cite{H264_Overview} codec was designed to reach a significant improvement regarding the distortion efficiency compared to existing standards. Efforts have been enhanced compression rate and a network-friendly representation for conversational and non-conversational applications. It also aimed to be flexible to be used on many different system types.\\
At the time, May 2003, parallelism of the algorithm H.264 itself wasn't really taken into consideration. Due to the fact that a standard personal computer around 2003 didn't have a multi core processor, parallelism wasn't considered important enough. The utilization of GPUs was also mostly limited to video gaming. Frameworks like CUDA \footnote{Compute Unified Device Architecture} or OpenCL\footnote{Open Computing Language} were just developed recently.\\
The H.265/HEVC \cite{H265_Overview} standard is the most recent project\footnote{released 2013} of the ITU-T Video Coding Experts Group (VCEG) and the ISO/IEC Moving
Picture Experts Group (MPEG) standardization organizations. The codec aims to reach half the size of the existing standards while maintaining the same quality, therefore significantly improve the compression rate. \todo{This goal is mainly reached by optimizing the use of parallel processing architectures, why using parallel architecture can reduce the bitrate by half, not true, remove it or find the truth}.\\
Both codecs are very similar and inherited the known hybrid Motion Estimation (ME)/Motion Compensation (MC) followed by transform and entropy coding framework adopted by H.261 \cite{H261_Overview} since 1994.\\

For H.264/AVC being available since 2003, a lot of work has been contributed to optimize parallelism for the algorithm. The Motion Estimation (ME) is one of the most compute intensive parts of both H.264 and H.265. A lot of studies have been made over the last years to \todo{research propose strategies, typo or grammar problem?} to utilize GPUs to undertake ME. \\
In \cite{ME1} a multi-pass method is proposed to unroll and rearrange the nested loops of ME and implement it on the GPU. They reached a speed-up of 2 to 14 times respectively for integer-pel ME and half-pel ME.\\

The authors of \cite{ME2} performed ME on a block-by-block basis on the GPU to overcome the dependency problem between blocks, which is often ignored by algorithms. The study group compared their GPU algorithm with a self optimized Single Instruction Multiple Data (SIMD) algorithm for CPU. They reached a speed-up of up to 10 times, but it has to be taken into account, that their algorithm has to be optimized for each GPU differently.\\

The article \cite{ME3} proposed an adoption of a parallel small diamond search on the GPU to speed-up the ME. They were greatly limited by the memory bandwidth but reached a small speed-up around 10\% but with a small loss in quality.\\

In \cite{ME4} they developed a scalable ME algorithm for GPUs that focusses on the computation power and takes re-usability of data into account. The scalability regarding different number of cores for  GPUs of the same architecture was also an important goal for the study to be used in the future. They reached speed-ups up to 3 times for their optimized algorithm on GPU.\\

Another approach, \cite{deblock1}, considered the possibility to parallelize the deblocking filter \todo{good to mention that it is for H264 deblocking filter, but not HEVC, as H264 is deblocking filter more challenging for GPU, due to dependency} on GPU. Usually deblocking is done on the CPU in serial due to its data dependencies. The articles approach tries to reduce these dependencies to make use of the parallel computation power of GPUs. For their implemented deblocking filter they reached speed-ups around 10 to 20 times compared to a state-of-the-art CPU deblocking filter.\\

In this paper three approaches are reviewed, which try to exploit the possible parallel performance of modern GPUs for the modern video codecs H.264 and H.265. \\
In section \ref{sec:background} a short overview on the two video coding techniques is given. In the following section \ref{sec:paper} the three papers \cite{Paper1}, \cite{Paper2} and \cite{Paper3} and their proposals are presented in detail. After that the experimental results of the papers are presented and compared in section \ref{sec:results}.\\
In the last section \ref{sec:conclusion} a conclusion is given.